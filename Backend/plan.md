SageSearch Backend Implementation Plan

We propose a Node.js + Express backend that uses a Retrieval-Augmented (Context-Augmented) Generation pipeline. User queries are embedded and matched against a vector database of preprocessed text passages (Bhagavad Gita, Kalam’s books, Vivekananda’s works). The API will return persona-based answers generated by an LLM using the retrieved context. The backend will consist of routes for persona selection and question-answering, a vector store for passage retrieval, and integration with an LLM API.

1. Data Acquisition & Preprocessing

Bhagavad Gita: Use a public-domain translation. For example, Project Gutenberg’s Bhagavad-Gita is freely available
gutenberg.org
. Download the text (e.g. Sir Edwin Arnold’s translation) and split it into logical passages (e.g. chapter/verse or paragraph-level segments).

Wings of Fire & Ignited Minds (Dr. A.P.J. Kalam): These are not public domain, but scans/texts exist online. For instance, Wings of Fire is available as a PDF on an official site
ati.dae.gov.in
, and Ignited Minds appears in the Internet Archive’s Public Library of India collection
archive.org
. We can obtain PDF/text versions (with permission or under fair-use for research) and OCR or clean them. Then split into chapters or paragraphs. For example, extract the text from page scans, remove headers/footers, and split by chapter.

Complete Works of Swami Vivekananda: Vivekananda’s works (died 1902) are public domain. They are available online (e.g. the Complete Works on Ramakrishna‑Vivekananda.info
ramakrishnavivekananda.info
 or Wikisource). Download or scrape the HTML/PDF of each volume, clean formatting, and split into passages (e.g. per lecture/lecture section).

Each source should be preprocessed into chunks of modest size (e.g. 500–1000 tokens). Common strategies include splitting by paragraph or fixed-length token windows with overlap
medium.com
. Store each chunk with metadata (source title, chapter, etc.) so that answers can be traced back. For example, a PostgreSQL table or JSON file could index {id, book, chapter, text}, while the same passage text is embedded and stored in the vector DB.

2. Vector Storage & Semantic Retrieval

To support semantic search, we will embed all text passages and load them into a vector database. Popular options include Qdrant, Chroma, and FAISS
learnopencv.com
.

Local vs. Hosted:

Local: Tools like FAISS (Facebook AI Similarity Search) or Chroma (Chromadb) can be self-hosted. FAISS is very fast for large indexes, and Chroma is easy to use in Python, but integrating them in a pure Node environment is tricky. For example, deploying Node.js with faiss-node on Render/Vercel often fails due to native library dependencies (e.g. libomp not found)
stackoverflow.com
. Chroma requires a separate Python service and Docker (which Render can do, but adds complexity).

Hosted: A managed vector service like Pinecone or Qdrant Cloud is easier. Both have Node.js client libraries and free tiers. Pinecone is fully managed (no infra to maintain), while Qdrant can be self-hosted in a container or used via their cloud offering
learnopencv.com
. Given student deployment constraints, a hosted solution like Pinecone or Qdrant Cloud is recommended to avoid low-level setup issues.

Once chosen, the flow is: generate embeddings (e.g. using OpenAI’s text-embedding models or a local embedder) for each chunk, and upsert them into the vector DB with their metadata. At runtime, for each query, we embed the query and perform an approximate nearest-neighbor search to retrieve the top-k most relevant passages. This keeps the LLM’s context small and focused on relevant information
learnopencv.com
.

3. API Design & Routes

We will expose a REST API (no frontend). Key routes include:

GET /personas – List available personas (e.g. “Kalam”, “Vivekananda”, “Bhagavad Gita” style, etc.). This allows a client to present choices.

POST /ask – Main query endpoint. Request body contains { personaId, question }. The handler will:

Look up the persona’s prompt template (see next section).

Embed the user question and perform a semantic search in the vector DB to retrieve top-N passages (e.g. 3–5 chunks).

Construct an LLM prompt using the persona system prompt + retrieved passages + user query.

Call the LLM API to generate an answer (see §4).

Return a response JSON including the answer and optionally references (which passages were used).

No session state or user accounts are needed; each request is independent. For simplicity, /ask can combine persona and retrieval in one call. We may also add a GET /health for status checks.

4. LLM Integration

We recommend using a conversational API such as OpenAI’s ChatCompletion (GPT-3.5 Turbo) for highest quality-to-cost. OpenAI offers a Node SDK (the openai NPM package) that lets us call openai.chat.completions.create({model, messages}) from Express
naveen-v-v.medium.com
. For example:

const response = await openai.chat.completions.create({
  model: 'gpt-3.5-turbo',
  messages: messagesArray  // includes system/persona + user query
});


GPT-3.5-Turbo is cost-effective and easy to use; if budgets allow, GPT-4 can be used for more nuanced answers. As an alternative, one could use a free/open model via Hugging Face or Replicate (e.g. Llama2/3), but that typically requires GPU hosting or API calls (and may underperform compared to OpenAI’s models). Cohere or Anthropic APIs are also options. We lean toward OpenAI or another well-documented API for ease of integration (they supply Node examples and good documentation).

5. Persona Management & Prompt Templates

Each persona (e.g. “Kalam”, “Vivekananda”, “Krishna” etc.) will have a system/preamble prompt that guides tone and style. Store personas in a JSON file or DB table like personas(id, name, prompt_template). For example:

Dr. Kalam persona: “You are Dr. A.P.J. Abdul Kalam, former President of India and scientist, known for your inspiring, educational style. Answer the question using a respectful, motivational tone. Use only the information from the provided passages.”

Vivekananda persona: “You are Swami Vivekananda, the 19th-century spiritual leader. Speak in a wise and uplifting manner, referring to the scriptures. Use only the provided context.”

At runtime, the server loads the appropriate template and includes it as a “system” message when calling the LLM. We’ll also explicitly add instructions for hallucination control (see §6). For instance, the full prompt might be:

System: You are <PersonaName> with style X. Refer only to the given passages.
User: Question: "<user question>" 
Context: "<retrieved passage 1> ... <passage N>"


No additional route is needed for persona selection beyond reading this template.

6. Hallucination Mitigation & Post-Processing

To reduce “hallucinations,” we will instruct the LLM to rely only on the retrieved passages and to admit uncertainty if needed. As recent RAG literature advises, prompts should explicitly ground the model in the sources
mindee.com
. For example, include in the system prompt: “Use only the information from the above passages. If the answer is not contained in them, say you don’t know.” We also allow “uncertainty modeling” so that the model can respond with “I don’t know” rather than invent facts
mindee.com
.

After the LLM generates an answer, we can do simple checks. For example, we might re-embed key claims in the answer to ensure they appear in retrieved context (or run a second, stricter retrieval). If an answer contains unsupported facts, we could flag it or trim it. In practice, clear prompt engineering is the first defense: by forcing the model to cite the passages or say “I’m not sure,” we minimize confident hallucinations
mindee.com
.

7. Project Structure & Database Schema

A clear folder structure will help the team. For example:

/src
  app.js                 # main Express app
  /routes
    personas.js          # GET /personas
    chat.js              # POST /ask (semantic search + LLM)
  /services
    embedder.js         # functions to embed queries/text
    vectorStore.js      # interface to vector DB (search, upsert)
    llm.js              # functions to call the LLM API
  /data
    texts/              # raw text files (Bhagavad Gita, etc.)
    chunks/             # preprocessed chunks or embeddings (optional)
  /config
    default.json        # config (API keys, db credentials)


Database: We can use a small PostgreSQL (available on Render/Railway) or even SQLite for persona data. A simple schema:

personas(id, name, prompt_template).

(Optional) passages(id, book, chapter, text) if we want a relational reference for passages. Otherwise, the vector DB holds the text in its metadata.

(No user table needed unless multi-user sessions are planned.)

Embeddings are stored in the vector DB. If using Qdrant or Pinecone, no separate storage is needed. We just tag each vector with metadata (e.g. { book: "Bhagavad Gita", chapter: 1 }).

8. Deployment

We will deploy on a platform like Render or Railway. Both support Node.js (and Docker if needed). Environment variables will hold secrets (OpenAI API key, vector DB keys). We’ll document setup clearly (e.g. “Set OPENAI_API_KEY, PINECONE_API_KEY, etc.”). Render can also host a PostgreSQL instance for personas.

During build/deploy, we will run a one-time job or script to preprocess texts and populate the vector store (this can be a CLI script or run at start if needed). The API itself will just query the already-populated index.

Monitoring: A simple health check endpoint and logging (to console or a log service) will help in debugging. Since this is a student project, we will keep it lightweight, focusing on clear code and docs.

9. References and Tools

Text Sources: We will cite our sources of text data. For example, Project Gutenberg’s Bhagavad Gita
gutenberg.org
, the ATI PDF for Wings of Fire
ati.dae.gov.in
, and the Internet Archive for Ignited Minds
archive.org
. Vivekananda’s works are available on Ramakrishna Vivekananda.info
ramakrishnavivekananda.info
.

Vector DB: We note Qdrant, Chroma, and FAISS as popular options
learnopencv.com
, but we will likely choose Pinecone or Qdrant Cloud to simplify deployment.

LLM Integration: Using OpenAI’s Node SDK with Chat Completions (e.g. model gpt-3.5-turbo) is straightforward
naveen-v-v.medium.com
 and cost-effective.

Hallucination Control: We follow best practices like those summarized in recent RAG guides
mindee.com
 by instructing the model to stay grounded in the provided context and by allowing it to say “I don’t know.”

Each component will be documented with inline comments and README instructions. This plan balances simplicity (well-supported APIs and services) with the needed functionality (semantic search, persona prompts, reliable answers) to build a fully deployable SageSearch backend.

Sources: Implementation references include vector DB guides
learnopencv.com
, deployment notes
stackoverflow.com
, OpenAI Node examples
naveen-v-v.medium.com
, and RAG best practices
mindee.com
. Data sources and preprocessing methods are drawn from public-domain and licensed texts
gutenberg.org
ati.dae.gov.in
archive.org
ramakrishnavivekananda.info
.